<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width">
  <title>Lyra V2 Web/Browser Demo</title>
  
  <script src="./enable-threads.js"></script>
</head>
<body>
  <!-- <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core@3.20.0"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-cpu@3.20.0"></script> -->
  
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.20.0/dist/tf.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-tflite@0.0.1-alpha.9/dist/tf-tflite.js"></script>
  
  <button id="testButton1" onclick="testEncoderDecoder(); testButton1.disabled=true; testButton2.disabled=true;">Test Encoder/Decoder</button>
  <button id="testButton2" onclick="testQuantization(); testButton1.disabled=true; testButton2.disabled=true;">Test Quantization</button>
  <p>See browser console for result. Note that this is a WIP - it isn't working yet!</p>
  
  <script>
    let encoderModel;
    let decoderModel;
    let quantizerModel;

    async function initModels(opts={}) {
      console.log("Loading models...");
      [
        encoderModel,
        quantizerModel,
        decoderModel,
      ] = await Promise.all([
        opts.encoder ? tflite.loadTFLiteModel("https://huggingface.co/rocca/lyra-v2-soundstream/resolve/main/tflite/soundstream_encoder.tflite") : undefined,
        opts.quantization ? tflite.loadTFLiteModel("https://huggingface.co/rocca/lyra-v2-soundstream/resolve/main/tflite/quantizer.tflite") : undefined,
        opts.decoder ? tflite.loadTFLiteModel("https://huggingface.co/rocca/lyra-v2-soundstream/resolve/main/tflite/lyragan.tflite") : undefined,
      ]);
      console.log("Models finished loading.");
    }

    async function encode(audioSamples) { // `audioSamples` should be a 320-element Float32Array
      if(!encoderModel) throw new Error("You need to init the encoder model first.");

      let audioSamplesTensor = tf.tensor(audioSamples, [1,320], "float32");

      let outputTensor = encoderModel.predict({"serving_default_input_audio:0":audioSamplesTensor});

      return outputTensor; // float32[1,1,64]
    }

    async function quantize(embedding, numQuantizers=46) { // `embedding` should be a tensor of type float32[1,1,64]. `numQuantizers` can be 16, 32, or 46. Explanation: https://github.com/google/lyra/issues/92#issuecomment-1265883697
      if(!quantizerModel) throw new Error("You need to init the quantizer model first.");

      let outputs = quantizerModel.predict({"encode_input_frames:0":embedding, "encode_num_quantizers:0":numQuantizers}); // Need to work out how to select the encoder subgraph: https://github.com/tensorflow/tfjs/issues/6919

      return outputs["StatefulPartitionedCall_1:0"]; // int32[46,1,1]
    }

    async function dequantize(bits) { // `bits` should be a tensor of type int32[46,1,1]
      if(!quantizerModel) throw new Error("You need to init the quantizer model first.");

      let outputs = quantizerModel.predict({"decode_encoding_indices:0":bits});

      return outputs["StatefulPartitionedCall:0"]; // float32[1,1,64]
    }

    async function decode(embedding) { // `embedding` should be a tensor of type float32[1,1,64]
      if(!decoderModel) throw new Error("You need to init the decoder model first.");

      let outputTensor = decoderModel.predict({"serving_default_input_audio:0":embedding});
      return outputTensor["StatefulPartitionedCall:0"].dataSync(); // a 320-element Float32Array
    }

    let audioCache = {};
    async function getAudioSamples(url, chunkI) {
      if(!audioCache[url]) {

        // Need audio length to set max length of OfflineAudioContext - (hacky)
        let audioDuration = await new Promise(r => {
          let audio = new Audio();
          audio.onloadedmetadata = () => r(audio.duration);
          audio.src = url;
        });

        let audioData = await fetch(url).then(r => r.arrayBuffer());

        let audioCtx = new OfflineAudioContext({sampleRate:16000, length:16000*audioDuration + 1000, numberOfChannels:1});

        let decodedData = await audioCtx.decodeAudioData(audioData); // resampled to the AudioContext's sampling rate

        let float32Data = decodedData.getChannelData(0); // Float32Array for channel 0

        audioCache[url] = float32Data;
      }

      let audioChunk = audioCache[url].slice(chunkI*320, (chunkI+1)*320); // 320 samples at 16khz = 20ms
      
      return audioChunk;
    }

    async function testEncoderDecoder() {
      console.log("Testing encoding/decoding:");
      console.warn("CURRENTLY WE CAN ONLY TEST ENCODER DUE TO THIS ISSUE: https://github.com/tensorflow/tfjs/issues/6094#issuecomment-1267870990");
      await initModels({encoder:true});
      let inputSamples = await getAudioSamples("https://huggingface.co/rocca/lyra-v2-soundstream/resolve/main/test.wav", 0);
      console.log("input samples:", inputSamples);
      let embedding = await encode(inputSamples);
      console.log("embedding:", embedding.dataSync());
      console.warn("Not sure why it's all zeros...");
      
      // let outputSamples = await decode(embedding);
      // console.log("output samples:", outputSamples);
    }
    
    async function testQuantization() {
      console.log("Testing quantization:");
      console.warn("DOESN'T WORK - BLOCKED BY: https://github.com/tensorflow/tfjs/issues/6919");
      await initModels({quantization:true});
      let inputEmbedding = tf.tensor(new Float32Array(64).fill(1), [1,1,64], "float32");
      console.log("input embedding:", inputEmbedding.dataSync());
      let bits = await quantize(inputEmbedding, 46);
      let outputEmbedding = dequantize(bits);
      console.log("output embedding:", outputEmbedding.dataSync());
    }

  </script>
</body>
</html>
